{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbcaa4f",
   "metadata": {},
   "source": [
    "## PyTorch implemtation of the CBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76bead",
   "metadata": {},
   "source": [
    "This notebook gives a brief introduction to the consensus-based optimization for training PyTorch neural networks. It covers the typical `torch` training loop and how to integrate it with 'Weights and biases' (`wandb`). Training is performed for the canonical MNIST dataset and a shallow network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd539a5",
   "metadata": {},
   "source": [
    "First, we impored the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b06fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e465cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c977c4",
   "metadata": {},
   "source": [
    "To import the library modules, we neeed to add the path to the root folder to `sys.path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af64de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join(os.getcwd().split('cbo-in-python')[0], 'cbo-in-python')\n",
    "\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e77e99",
   "metadata": {},
   "source": [
    "First, we import a function for loading the train and test MNIST dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7983d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import load_mnist_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34a429",
   "metadata": {},
   "source": [
    "For convenience, in `src.torch.models` we provide a few model architectures for user experiments. Surely, one may also implement a neural network from scratch (using the `torch.nn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57ff689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch.models import SmallMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f37e06",
   "metadata": {},
   "source": [
    "Now, we import two remaining classes for performing the CBO optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f23b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch import Optimizer, Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc501",
   "metadata": {},
   "source": [
    "One may use cuda for accelerated computations. The command below will determine the computational device (cuda or CPU) based on the availability of the cuda. It is a standard PyTorch way of doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a0be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae57e7",
   "metadata": {},
   "source": [
    "Now, we can load train and test dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3945ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60  # samples-level batching\n",
    "\n",
    "train_dataloader, test_dataloader = load_mnist_dataloaders(train_batch_size=batch_size,\n",
    "                                                           test_batch_size=batch_size)\n",
    "\n",
    "n_batches = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa3d0a",
   "metadata": {},
   "source": [
    "Now, we define the remaining optimization params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "369d0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "particles_batch_size = 10\n",
    "\n",
    "n_particles = 100\n",
    "alpha = 50\n",
    "sigma = 0.4 ** 0.5\n",
    "l = 1\n",
    "dt = 0.1\n",
    "anisotropic = True\n",
    "eps = 1e-2\n",
    "\n",
    "partial_update = False\n",
    "\n",
    "use_multiprocessing=False\n",
    "\n",
    "eval_freq = 100  # how often to evaluate the validation (test) accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533e3c3",
   "metadata": {},
   "source": [
    "Now, we define two helper functions to perform the training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce20273",
   "metadata": {},
   "source": [
    "Function `evaluate` is used to calculate the model accuracy on the current batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12fdd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f216c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_, y_):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_)\n",
    "        y_pred = torch.argmax(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, y_)\n",
    "        acc = accuracy(y_pred.cpu(), y_.cpu())\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a338a",
   "metadata": {},
   "source": [
    "Function `log` is used to log the metrics to `wandb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db21c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(loss, acc, epoch, stage='train', shift_norm=None):\n",
    "    wandb.log({\n",
    "        f'{stage}_loss': loss,\n",
    "        f'{stage}_acc': acc,\n",
    "        'epoch': epoch,\n",
    "        f'{stage}_shift_norm': shift_norm,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e611d",
   "metadata": {},
   "source": [
    "'Weights and biases' (`wandb`) is an experiment tracking tool for machine learning. Please refer to the [official website](https://wandb.ai/site) for more details. The command bellow will initialize the current experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a372a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='CBO', entity='itukh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f705b5d",
   "metadata": {},
   "source": [
    "We will use the provided shallow `SmallMLP` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb1f735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallMLP(\n",
       "  (model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=3)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (5): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (7): LogSoftmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallMLP().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084744e0",
   "metadata": {},
   "source": [
    "In order to perform the optimization, we need to define:\n",
    "* `optimizer` (`src.torch.Optimizer`);\n",
    "* `loss_fn` (`src.torch.Loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80600a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(model, n_particles=n_particles, alpha=alpha, sigma=sigma,\n",
    "                      l=l, dt=dt, anisotropic=anisotropic, eps=eps, partial_update=partial_update,\n",
    "                      use_multiprocessing=use_multiprocessing,\n",
    "                      particles_batch_size=particles_batch_size, device=device)\n",
    "loss_fn = Loss(F.nll_loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57caf2c",
   "metadata": {},
   "source": [
    "Now, let's proceed with the final training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # main loop over the training epochs\n",
    "    for batch, (X, y) in enumerate(train_dataloader):  # nested loop over the batches of training samples\n",
    "        X, y = X.to(device), y.to(device)  # we convert data samples to the device   \n",
    "        train_loss, train_acc = evaluate(model, X, y)\n",
    "        log(train_loss, train_acc, epoch)  # log the metrics to wandb\n",
    "        loss_fn.backward(X, y)  # use the current training data batch\n",
    "        optimizer.step()  # optimization step\n",
    "        \n",
    "        if batch % eval_freq == 0 or batch == n_batches - 1:  # evaluate the test accuracy\n",
    "            with torch.no_grad():\n",
    "                losses = []\n",
    "                accuracies = []\n",
    "                for X_test, y_test in test_dataloader:\n",
    "                    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                    loss, acc = evaluate(model, X_test, y_test)\n",
    "                    losses.append(loss.cpu())\n",
    "                    accuracies.append(acc.cpu())\n",
    "                val_loss, val_acc = np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "            print(f'Epoch: {epoch + 1:2}/{epochs}, batch: {batch + 1:4}/{n_batches}, train loss: {train_loss:8.3f}, train acc: {train_acc:8.3f}, val loss: {val_loss:8.3f}, val acc: {val_acc:8.3f}',\n",
    "                  end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb5e96",
   "metadata": {},
   "source": [
    "Please refer to [this folder](https://github.com/Igor-Tukh/cbo-in-python/tree/master/notebooks/experiments) for more advanced usage examples (for instance, for using the gamma term)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
