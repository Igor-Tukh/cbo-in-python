{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implemtation of the CBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives a brief introduction to the consensus-based optimization for the `PyTorch` framework. It covers the typical `torch` training loop and how to integrate it with 'Weights and biases' (`wandb`). Training is performed for the canonical MNIST dataset and a shallow network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "\n",
    "sys.path.extend([os.pardir,\n",
    "                 os.path.join(os.pardir, os.pardir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import load_mnist_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(1, 3),\n",
    "    nn.Linear(28 ** 2, 10),\n",
    "    nn.BatchNorm1d(10, affine=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.BatchNorm1d(10, affine=False),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LogSoftmax(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `wandb`. You will need to set the value of `entity` to your `wandb` login (you need to create the account firts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s23k12yq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▃▂▃▃▁▁▄▃▄▄▆▃▃▇▄▇▄▅▅▅▇▆▆▆▇▅█▄▅▅▅▅▆▅█▇▆▅▇</td></tr><tr><td>train_loss</td><td>▃█▆▅▃▃▃▂▂▂▃▁▂▂▁▂▁▂▂▂▂▁▁▁▂▁▂▁▂▂▂▂▂▁▁▁▁▂▂▁</td></tr><tr><td>train_shift_norm</td><td>██▆▄▇▄▃▃▃▂▂▃▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▃▂▁▂▂▃▂▂▂▄▄▄▄▄▄▄▄▄▄▄▃▅▇▇▆▇▆▆▆▆▆▆▆▇▇█████</td></tr><tr><td>val_loss</td><td>██▅▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_acc</td><td>0.35</td></tr><tr><td>train_loss</td><td>1.85872</td></tr><tr><td>train_shift_norm</td><td>0.07547</td></tr><tr><td>val_acc</td><td>0.3487</td></tr><tr><td>val_loss</td><td>1.92595</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">still-frost-39</strong>: <a href=\"https://wandb.ai/itukh/CBO/runs/s23k12yq\" target=\"_blank\">https://wandb.ai/itukh/CBO/runs/s23k12yq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220620_172528-s23k12yq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s23k12yq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/itukh/TUM/CBO/cbo-in-tensorflow/notebooks/wandb/run-20220620_173054-22yqytlb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/itukh/CBO/runs/22yqytlb\" target=\"_blank\">woven-water-40</a></strong> to <a href=\"https://wandb.ai/itukh/CBO\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/itukh/CBO/runs/22yqytlb?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f780c3786d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='CBO', entity='itukh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charts of your training above will be updated during the training process in real time. You can view them by the link specified in the previous cell output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify values of training and CBO hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "epochs = 50\n",
    "batch_size = 60\n",
    "# CBO params\n",
    "n_particles = 100\n",
    "alpha = 50\n",
    "l = 1  # lambda\n",
    "sigma = 0.4 ** 0.5\n",
    "dt = 0.1\n",
    "anisotropic = True\n",
    "eps = 5e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a consensus-based optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch import Optimizer\n",
    "\n",
    "optimizer = Optimizer(model, n_particles=n_particles, alpha=alpha, sigma=sigma,\n",
    "                      l=l, dt=dt, anisotropic=anisotropic, eps=eps, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a wrapper for the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch import Loss\n",
    "\n",
    "loss_fn = Loss(F.nll_loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above `F.nll_loss` is the standard `torch` implemtation of negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the MNIST dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = load_mnist_dataloaders(train_batch_size=batch_size,\n",
    "                                                           test_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the `wandb` config to save the hyperparameter values for the current run. In principle, it is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  'epochs': epochs,\n",
    "  'batch_size': batch_size,\n",
    "    \n",
    "  'n_particles': n_particles,\n",
    "  'alpha': alpha,\n",
    "  'lambda': l,\n",
    "  'sigma': sigma,\n",
    "  'dt': dt,\n",
    "  'eps': eps,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write helper functions to evalueate your model and log the results into `wandb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy()\n",
    "\n",
    "def evaluate(model, X_, y_):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_)\n",
    "        y_pred = torch.argmax(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, y_)\n",
    "        acc = accuracy(y_pred, y_)\n",
    "    return loss, acc\n",
    "\n",
    "def log(loss, acc, epoch, stage='train', shift_norm=None):\n",
    "    wandb.log({\n",
    "        f'{stage}_loss': loss,\n",
    "        f'{stage}_acc': acc,\n",
    "        'epoch': epoch,\n",
    "        f'{stage}_shift_norm': shift_norm,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training looop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itukh/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/50, batch:   74/1000, train loss:    1.751, train acc:    0.417, val loss:    1.888, val acc:    0.347\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        train_loss, train_acc = evaluate(model, X, y)\n",
    "        log(train_loss, train_acc, epoch, shift_norm=optimizer.shift_norm)\n",
    "        optimizer.zero_grad()  # optional\n",
    "        loss_fn.backward(X, y)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            for X_test, y_test in test_dataloader:\n",
    "                loss, acc = evaluate(model, X_test, y_test)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(acc)\n",
    "            val_loss, val_acc = np.mean(losses), np.mean(accuracies)\n",
    "            log(val_loss, val_acc, epoch, 'val')\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1:2}/{epochs}, batch: {batch + 1:4}/{n_batches}, train loss: {train_loss:8.3f}, train acc: {train_acc:8.3f}, val loss: {val_loss:8.3f}, val acc: {val_acc:8.3f}',\n",
    "              end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
